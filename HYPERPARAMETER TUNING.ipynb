{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15362f93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T13:42:45.243962Z",
     "iopub.status.busy": "2024-06-17T13:42:45.243611Z",
     "iopub.status.idle": "2024-06-17T23:33:07.807864Z",
     "shell.execute_reply": "2024-06-17T23:33:07.806628Z"
    },
    "papermill": {
     "duration": 35422.604365,
     "end_time": "2024-06-17T23:33:07.843610",
     "exception": false,
     "start_time": "2024-06-17T13:42:45.239245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 1 - Epoch [1/30] - Avg Train Loss: 0.0186, Val Loss: 0.0124, PSNR: 21.2690\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 1 - Epoch [2/30] - Avg Train Loss: 0.0101, Val Loss: 0.0108, PSNR: 20.9596\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 1 - Epoch [3/30] - Avg Train Loss: 0.0090, Val Loss: 0.0103, PSNR: 21.2878\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 1 - Epoch [4/30] - Avg Train Loss: 0.0084, Val Loss: 0.0098, PSNR: 21.9843\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 1 - Epoch [5/30] - Avg Train Loss: 0.0081, Val Loss: 0.0095, PSNR: 22.1693\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 1 - Epoch [6/30] - Avg Train Loss: 0.0080, Val Loss: 0.0102, PSNR: 21.8825\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 1 - Epoch [7/30] - Avg Train Loss: 0.0076, Val Loss: 0.0092, PSNR: 21.9760\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 1 - Epoch [8/30] - Avg Train Loss: 0.0073, Val Loss: 0.0096, PSNR: 22.4214\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 1 - Epoch [9/30] - Avg Train Loss: 0.0071, Val Loss: 0.0089, PSNR: 22.1834\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 1 - Epoch [10/30] - Avg Train Loss: 0.0069, Val Loss: 0.0084, PSNR: 22.6558\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 1 - Epoch [11/30] - Avg Train Loss: 0.0066, Val Loss: 0.0078, PSNR: 23.0126\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 1 - Epoch [12/30] - Avg Train Loss: 0.0063, Val Loss: 0.0080, PSNR: 22.9298\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 1 - Epoch [13/30] - Avg Train Loss: 0.0063, Val Loss: 0.0078, PSNR: 22.7859\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 1 - Epoch [14/30] - Avg Train Loss: 0.0060, Val Loss: 0.0077, PSNR: 23.0967\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 1 - Epoch [15/30] - Avg Train Loss: 0.0057, Val Loss: 0.0070, PSNR: 23.4897\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 1 - Epoch [16/30] - Avg Train Loss: 0.0054, Val Loss: 0.0068, PSNR: 23.3718\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 1 - Epoch [17/30] - Avg Train Loss: 0.0052, Val Loss: 0.0075, PSNR: 23.2558\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 1 - Epoch [18/30] - Avg Train Loss: 0.0052, Val Loss: 0.0068, PSNR: 23.3398\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 1 - Epoch [19/30] - Avg Train Loss: 0.0050, Val Loss: 0.0075, PSNR: 22.9387\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 1 - Epoch [20/30] - Avg Train Loss: 0.0050, Val Loss: 0.0070, PSNR: 23.0354\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 1 - Epoch [21/30] - Avg Train Loss: 0.0047, Val Loss: 0.0073, PSNR: 24.0461\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 1 - Epoch [22/30] - Avg Train Loss: 0.0046, Val Loss: 0.0072, PSNR: 23.6030\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 1 - Epoch [23/30] - Avg Train Loss: 0.0046, Val Loss: 0.0072, PSNR: 23.8246\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 1 - Epoch [24/30] - Avg Train Loss: 0.0045, Val Loss: 0.0075, PSNR: 23.4119\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 1 - Epoch [25/30] - Avg Train Loss: 0.0043, Val Loss: 0.0068, PSNR: 23.2864\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 1 - Epoch [26/30] - Avg Train Loss: 0.0043, Val Loss: 0.0068, PSNR: 23.7482\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 1 - Epoch [27/30] - Avg Train Loss: 0.0042, Val Loss: 0.0069, PSNR: 23.9990\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 1 - Epoch [28/30] - Avg Train Loss: 0.0042, Val Loss: 0.0065, PSNR: 23.8351\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 1 - Epoch [29/30] - Avg Train Loss: 0.0041, Val Loss: 0.0070, PSNR: 23.4064\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 1 - Epoch [30/30] - Avg Train Loss: 0.0042, Val Loss: 0.0072, PSNR: 23.9996\n",
      "Hyperparameters (LR: 0.0001, Batch Size: 4, Accumulation Steps: 1) - Average PSNR over 30 epochs: 22.9069\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 4 - Epoch [1/30] - Avg Train Loss: 0.0378, Val Loss: 0.0165, PSNR: 18.8520\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 4 - Epoch [2/30] - Avg Train Loss: 0.0176, Val Loss: 0.0142, PSNR: 20.1142\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 4 - Epoch [3/30] - Avg Train Loss: 0.0118, Val Loss: 0.0126, PSNR: 20.5325\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 4 - Epoch [4/30] - Avg Train Loss: 0.0100, Val Loss: 0.0104, PSNR: 21.4887\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 4 - Epoch [5/30] - Avg Train Loss: 0.0088, Val Loss: 0.0096, PSNR: 22.1607\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 4 - Epoch [6/30] - Avg Train Loss: 0.0085, Val Loss: 0.0094, PSNR: 21.4288\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 4 - Epoch [7/30] - Avg Train Loss: 0.0082, Val Loss: 0.0093, PSNR: 21.7338\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 4 - Epoch [8/30] - Avg Train Loss: 0.0078, Val Loss: 0.0090, PSNR: 21.7575\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 4 - Epoch [9/30] - Avg Train Loss: 0.0076, Val Loss: 0.0101, PSNR: 21.1915\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 4 - Epoch [10/30] - Avg Train Loss: 0.0073, Val Loss: 0.0088, PSNR: 22.2356\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 4 - Epoch [11/30] - Avg Train Loss: 0.0071, Val Loss: 0.0084, PSNR: 22.5950\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 4 - Epoch [12/30] - Avg Train Loss: 0.0069, Val Loss: 0.0096, PSNR: 21.7069\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 4 - Epoch [13/30] - Avg Train Loss: 0.0068, Val Loss: 0.0078, PSNR: 22.8208\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 4 - Epoch [14/30] - Avg Train Loss: 0.0064, Val Loss: 0.0080, PSNR: 22.9220\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 4 - Epoch [15/30] - Avg Train Loss: 0.0062, Val Loss: 0.0081, PSNR: 23.5136\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 4 - Epoch [16/30] - Avg Train Loss: 0.0059, Val Loss: 0.0070, PSNR: 23.5268\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 4 - Epoch [17/30] - Avg Train Loss: 0.0058, Val Loss: 0.0075, PSNR: 23.6897\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 4 - Epoch [18/30] - Avg Train Loss: 0.0054, Val Loss: 0.0073, PSNR: 22.7903\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 4 - Epoch [19/30] - Avg Train Loss: 0.0054, Val Loss: 0.0074, PSNR: 22.6817\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 4 - Epoch [20/30] - Avg Train Loss: 0.0052, Val Loss: 0.0066, PSNR: 23.9202\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 4 - Epoch [21/30] - Avg Train Loss: 0.0049, Val Loss: 0.0071, PSNR: 23.1800\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 4 - Epoch [22/30] - Avg Train Loss: 0.0051, Val Loss: 0.0071, PSNR: 23.0395\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 4 - Epoch [23/30] - Avg Train Loss: 0.0048, Val Loss: 0.0071, PSNR: 23.1207\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 4 - Epoch [24/30] - Avg Train Loss: 0.0046, Val Loss: 0.0069, PSNR: 23.7419\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 4 - Epoch [25/30] - Avg Train Loss: 0.0043, Val Loss: 0.0069, PSNR: 23.5529\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 4 - Epoch [26/30] - Avg Train Loss: 0.0042, Val Loss: 0.0071, PSNR: 23.5122\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 4 - Epoch [27/30] - Avg Train Loss: 0.0042, Val Loss: 0.0070, PSNR: 23.3580\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 4 - Epoch [28/30] - Avg Train Loss: 0.0041, Val Loss: 0.0066, PSNR: 23.2093\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 4 - Epoch [29/30] - Avg Train Loss: 0.0040, Val Loss: 0.0066, PSNR: 23.3459\n",
      "LR: 0.0001, Batch Size: 4, Accumulation Steps: 4 - Epoch [30/30] - Avg Train Loss: 0.0041, Val Loss: 0.0064, PSNR: 24.0074\n",
      "Hyperparameters (LR: 0.0001, Batch Size: 4, Accumulation Steps: 4) - Average PSNR over 30 epochs: 22.5243\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 1 - Epoch [1/30] - Avg Train Loss: 0.0254, Val Loss: 0.0182, PSNR: 18.7154\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 1 - Epoch [2/30] - Avg Train Loss: 0.0103, Val Loss: 0.0121, PSNR: 20.3688\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 1 - Epoch [3/30] - Avg Train Loss: 0.0086, Val Loss: 0.0109, PSNR: 20.4210\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 1 - Epoch [4/30] - Avg Train Loss: 0.0080, Val Loss: 0.0103, PSNR: 21.1315\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 1 - Epoch [5/30] - Avg Train Loss: 0.0075, Val Loss: 0.0102, PSNR: 21.1162\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 1 - Epoch [6/30] - Avg Train Loss: 0.0072, Val Loss: 0.0093, PSNR: 21.4622\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 1 - Epoch [7/30] - Avg Train Loss: 0.0070, Val Loss: 0.0083, PSNR: 22.3218\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 1 - Epoch [8/30] - Avg Train Loss: 0.0068, Val Loss: 0.0083, PSNR: 22.8511\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 1 - Epoch [9/30] - Avg Train Loss: 0.0064, Val Loss: 0.0087, PSNR: 22.0169\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 1 - Epoch [10/30] - Avg Train Loss: 0.0060, Val Loss: 0.0095, PSNR: 22.6619\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 1 - Epoch [11/30] - Avg Train Loss: 0.0057, Val Loss: 0.0090, PSNR: 22.6713\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 1 - Epoch [12/30] - Avg Train Loss: 0.0053, Val Loss: 0.0078, PSNR: 22.6499\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 1 - Epoch [13/30] - Avg Train Loss: 0.0052, Val Loss: 0.0094, PSNR: 21.6142\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 1 - Epoch [14/30] - Avg Train Loss: 0.0051, Val Loss: 0.0084, PSNR: 22.6425\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 1 - Epoch [15/30] - Avg Train Loss: 0.0049, Val Loss: 0.0087, PSNR: 22.7850\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 1 - Epoch [16/30] - Avg Train Loss: 0.0045, Val Loss: 0.0084, PSNR: 22.5490\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 1 - Epoch [17/30] - Avg Train Loss: 0.0044, Val Loss: 0.0088, PSNR: 23.1176\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 1 - Epoch [18/30] - Avg Train Loss: 0.0043, Val Loss: 0.0085, PSNR: 23.3538\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 1 - Epoch [19/30] - Avg Train Loss: 0.0039, Val Loss: 0.0088, PSNR: 23.3435\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 1 - Epoch [20/30] - Avg Train Loss: 0.0039, Val Loss: 0.0086, PSNR: 22.8179\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 1 - Epoch [21/30] - Avg Train Loss: 0.0039, Val Loss: 0.0083, PSNR: 23.4831\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 1 - Epoch [22/30] - Avg Train Loss: 0.0037, Val Loss: 0.0086, PSNR: 23.2340\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 1 - Epoch [23/30] - Avg Train Loss: 0.0038, Val Loss: 0.0085, PSNR: 23.5517\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 1 - Epoch [24/30] - Avg Train Loss: 0.0036, Val Loss: 0.0088, PSNR: 23.2726\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 1 - Epoch [25/30] - Avg Train Loss: 0.0035, Val Loss: 0.0090, PSNR: 23.2683\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 1 - Epoch [26/30] - Avg Train Loss: 0.0036, Val Loss: 0.0088, PSNR: 23.7828\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 1 - Epoch [27/30] - Avg Train Loss: 0.0035, Val Loss: 0.0089, PSNR: 23.4699\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 1 - Epoch [28/30] - Avg Train Loss: 0.0035, Val Loss: 0.0088, PSNR: 22.7747\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 1 - Epoch [29/30] - Avg Train Loss: 0.0034, Val Loss: 0.0085, PSNR: 23.2795\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 1 - Epoch [30/30] - Avg Train Loss: 0.0035, Val Loss: 0.0089, PSNR: 22.9858\n",
      "Hyperparameters (LR: 0.0001, Batch Size: 8, Accumulation Steps: 1) - Average PSNR over 30 epochs: 22.4571\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 4 - Epoch [1/30] - Avg Train Loss: 0.0418, Val Loss: 0.0228, PSNR: 16.9445\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 4 - Epoch [2/30] - Avg Train Loss: 0.0236, Val Loss: 0.0147, PSNR: 19.0340\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 4 - Epoch [3/30] - Avg Train Loss: 0.0161, Val Loss: 0.0118, PSNR: 20.2609\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 4 - Epoch [4/30] - Avg Train Loss: 0.0121, Val Loss: 0.0122, PSNR: 20.8270\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 4 - Epoch [5/30] - Avg Train Loss: 0.0100, Val Loss: 0.0103, PSNR: 21.2529\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 4 - Epoch [6/30] - Avg Train Loss: 0.0083, Val Loss: 0.0105, PSNR: 20.8040\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 4 - Epoch [7/30] - Avg Train Loss: 0.0076, Val Loss: 0.0080, PSNR: 22.4621\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 4 - Epoch [8/30] - Avg Train Loss: 0.0071, Val Loss: 0.0096, PSNR: 21.7940\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 4 - Epoch [9/30] - Avg Train Loss: 0.0067, Val Loss: 0.0086, PSNR: 21.9838\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 4 - Epoch [10/30] - Avg Train Loss: 0.0063, Val Loss: 0.0089, PSNR: 22.2201\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 4 - Epoch [11/30] - Avg Train Loss: 0.0063, Val Loss: 0.0096, PSNR: 22.4544\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 4 - Epoch [12/30] - Avg Train Loss: 0.0059, Val Loss: 0.0087, PSNR: 21.9570\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 4 - Epoch [13/30] - Avg Train Loss: 0.0059, Val Loss: 0.0105, PSNR: 22.0487\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 4 - Epoch [14/30] - Avg Train Loss: 0.0054, Val Loss: 0.0085, PSNR: 22.7594\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 4 - Epoch [15/30] - Avg Train Loss: 0.0051, Val Loss: 0.0090, PSNR: 22.7439\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 4 - Epoch [16/30] - Avg Train Loss: 0.0049, Val Loss: 0.0091, PSNR: 22.6870\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 4 - Epoch [17/30] - Avg Train Loss: 0.0046, Val Loss: 0.0075, PSNR: 23.1586\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 4 - Epoch [18/30] - Avg Train Loss: 0.0044, Val Loss: 0.0090, PSNR: 23.0739\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 4 - Epoch [19/30] - Avg Train Loss: 0.0043, Val Loss: 0.0094, PSNR: 22.6175\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 4 - Epoch [20/30] - Avg Train Loss: 0.0042, Val Loss: 0.0083, PSNR: 22.0806\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 4 - Epoch [21/30] - Avg Train Loss: 0.0041, Val Loss: 0.0082, PSNR: 23.1175\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 4 - Epoch [22/30] - Avg Train Loss: 0.0040, Val Loss: 0.0079, PSNR: 22.9394\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 4 - Epoch [23/30] - Avg Train Loss: 0.0039, Val Loss: 0.0088, PSNR: 23.2575\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 4 - Epoch [24/30] - Avg Train Loss: 0.0039, Val Loss: 0.0078, PSNR: 23.0811\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 4 - Epoch [25/30] - Avg Train Loss: 0.0038, Val Loss: 0.0084, PSNR: 23.4143\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 4 - Epoch [26/30] - Avg Train Loss: 0.0038, Val Loss: 0.0088, PSNR: 22.9864\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 4 - Epoch [27/30] - Avg Train Loss: 0.0037, Val Loss: 0.0082, PSNR: 23.3662\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 4 - Epoch [28/30] - Avg Train Loss: 0.0037, Val Loss: 0.0086, PSNR: 23.1369\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 4 - Epoch [29/30] - Avg Train Loss: 0.0036, Val Loss: 0.0085, PSNR: 23.3356\n",
      "LR: 0.0001, Batch Size: 8, Accumulation Steps: 4 - Epoch [30/30] - Avg Train Loss: 0.0037, Val Loss: 0.0084, PSNR: 23.2555\n",
      "Hyperparameters (LR: 0.0001, Batch Size: 8, Accumulation Steps: 4) - Average PSNR over 30 epochs: 22.1685\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 1 - Epoch [1/30] - Avg Train Loss: 0.0142, Val Loss: 0.0118, PSNR: 20.5732\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 1 - Epoch [2/30] - Avg Train Loss: 0.0103, Val Loss: 0.0104, PSNR: 21.7582\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 1 - Epoch [3/30] - Avg Train Loss: 0.0093, Val Loss: 0.0102, PSNR: 21.6714\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 1 - Epoch [4/30] - Avg Train Loss: 0.0087, Val Loss: 0.0101, PSNR: 21.8295\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 1 - Epoch [5/30] - Avg Train Loss: 0.0086, Val Loss: 0.0091, PSNR: 21.8678\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 1 - Epoch [6/30] - Avg Train Loss: 0.0080, Val Loss: 0.0084, PSNR: 22.6264\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 1 - Epoch [7/30] - Avg Train Loss: 0.0077, Val Loss: 0.0091, PSNR: 22.6333\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 1 - Epoch [8/30] - Avg Train Loss: 0.0075, Val Loss: 0.0089, PSNR: 22.6719\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 1 - Epoch [9/30] - Avg Train Loss: 0.0073, Val Loss: 0.0093, PSNR: 22.8551\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 1 - Epoch [10/30] - Avg Train Loss: 0.0071, Val Loss: 0.0086, PSNR: 22.1744\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 1 - Epoch [11/30] - Avg Train Loss: 0.0068, Val Loss: 0.0083, PSNR: 22.5000\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 1 - Epoch [12/30] - Avg Train Loss: 0.0066, Val Loss: 0.0085, PSNR: 22.7567\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 1 - Epoch [13/30] - Avg Train Loss: 0.0066, Val Loss: 0.0085, PSNR: 22.7002\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 1 - Epoch [14/30] - Avg Train Loss: 0.0062, Val Loss: 0.0085, PSNR: 22.5457\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 1 - Epoch [15/30] - Avg Train Loss: 0.0061, Val Loss: 0.0079, PSNR: 22.9871\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 1 - Epoch [16/30] - Avg Train Loss: 0.0059, Val Loss: 0.0086, PSNR: 23.2821\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 1 - Epoch [17/30] - Avg Train Loss: 0.0059, Val Loss: 0.0079, PSNR: 22.9875\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 1 - Epoch [18/30] - Avg Train Loss: 0.0056, Val Loss: 0.0080, PSNR: 23.1978\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 1 - Epoch [19/30] - Avg Train Loss: 0.0055, Val Loss: 0.0072, PSNR: 23.2186\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 1 - Epoch [20/30] - Avg Train Loss: 0.0054, Val Loss: 0.0082, PSNR: 22.6142\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 1 - Epoch [21/30] - Avg Train Loss: 0.0053, Val Loss: 0.0078, PSNR: 23.3540\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 1 - Epoch [22/30] - Avg Train Loss: 0.0052, Val Loss: 0.0072, PSNR: 23.6628\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 1 - Epoch [23/30] - Avg Train Loss: 0.0050, Val Loss: 0.0066, PSNR: 23.7611\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 1 - Epoch [24/30] - Avg Train Loss: 0.0050, Val Loss: 0.0069, PSNR: 23.8046\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 1 - Epoch [25/30] - Avg Train Loss: 0.0050, Val Loss: 0.0076, PSNR: 23.7515\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 1 - Epoch [26/30] - Avg Train Loss: 0.0047, Val Loss: 0.0070, PSNR: 23.9152\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 1 - Epoch [27/30] - Avg Train Loss: 0.0046, Val Loss: 0.0064, PSNR: 24.1596\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 1 - Epoch [28/30] - Avg Train Loss: 0.0046, Val Loss: 0.0066, PSNR: 23.7088\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 1 - Epoch [29/30] - Avg Train Loss: 0.0045, Val Loss: 0.0070, PSNR: 23.7766\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 1 - Epoch [30/30] - Avg Train Loss: 0.0044, Val Loss: 0.0068, PSNR: 24.0269\n",
      "Hyperparameters (LR: 0.0005, Batch Size: 4, Accumulation Steps: 1) - Average PSNR over 30 epochs: 22.9124\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 4 - Epoch [1/30] - Avg Train Loss: 0.0213, Val Loss: 0.0112, PSNR: 21.4179\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 4 - Epoch [2/30] - Avg Train Loss: 0.0103, Val Loss: 0.0106, PSNR: 21.5995\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 4 - Epoch [3/30] - Avg Train Loss: 0.0092, Val Loss: 0.0100, PSNR: 21.9358\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 4 - Epoch [4/30] - Avg Train Loss: 0.0089, Val Loss: 0.0103, PSNR: 21.8201\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 4 - Epoch [5/30] - Avg Train Loss: 0.0083, Val Loss: 0.0096, PSNR: 21.4869\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 4 - Epoch [6/30] - Avg Train Loss: 0.0077, Val Loss: 0.0097, PSNR: 22.2434\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 4 - Epoch [7/30] - Avg Train Loss: 0.0075, Val Loss: 0.0091, PSNR: 22.1503\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 4 - Epoch [8/30] - Avg Train Loss: 0.0075, Val Loss: 0.0086, PSNR: 22.6049\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 4 - Epoch [9/30] - Avg Train Loss: 0.0074, Val Loss: 0.0094, PSNR: 22.5204\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 4 - Epoch [10/30] - Avg Train Loss: 0.0073, Val Loss: 0.0098, PSNR: 22.7776\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 4 - Epoch [11/30] - Avg Train Loss: 0.0071, Val Loss: 0.0078, PSNR: 23.4356\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 4 - Epoch [12/30] - Avg Train Loss: 0.0068, Val Loss: 0.0089, PSNR: 23.2800\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 4 - Epoch [13/30] - Avg Train Loss: 0.0065, Val Loss: 0.0085, PSNR: 22.5358\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 4 - Epoch [14/30] - Avg Train Loss: 0.0066, Val Loss: 0.0087, PSNR: 22.2368\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 4 - Epoch [15/30] - Avg Train Loss: 0.0063, Val Loss: 0.0085, PSNR: 22.7050\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 4 - Epoch [16/30] - Avg Train Loss: 0.0059, Val Loss: 0.0082, PSNR: 23.0851\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 4 - Epoch [17/30] - Avg Train Loss: 0.0058, Val Loss: 0.0075, PSNR: 23.4728\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 4 - Epoch [18/30] - Avg Train Loss: 0.0059, Val Loss: 0.0086, PSNR: 22.7439\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 4 - Epoch [19/30] - Avg Train Loss: 0.0055, Val Loss: 0.0075, PSNR: 23.1813\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 4 - Epoch [20/30] - Avg Train Loss: 0.0055, Val Loss: 0.0072, PSNR: 23.3330\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 4 - Epoch [21/30] - Avg Train Loss: 0.0054, Val Loss: 0.0081, PSNR: 23.2904\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 4 - Epoch [22/30] - Avg Train Loss: 0.0054, Val Loss: 0.0069, PSNR: 23.4749\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 4 - Epoch [23/30] - Avg Train Loss: 0.0054, Val Loss: 0.0073, PSNR: 23.4968\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 4 - Epoch [24/30] - Avg Train Loss: 0.0052, Val Loss: 0.0069, PSNR: 23.6180\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 4 - Epoch [25/30] - Avg Train Loss: 0.0050, Val Loss: 0.0070, PSNR: 23.4985\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 4 - Epoch [26/30] - Avg Train Loss: 0.0050, Val Loss: 0.0077, PSNR: 22.9296\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 4 - Epoch [27/30] - Avg Train Loss: 0.0051, Val Loss: 0.0079, PSNR: 23.3911\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 4 - Epoch [28/30] - Avg Train Loss: 0.0050, Val Loss: 0.0077, PSNR: 23.4327\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 4 - Epoch [29/30] - Avg Train Loss: 0.0048, Val Loss: 0.0066, PSNR: 24.4014\n",
      "LR: 0.0005, Batch Size: 4, Accumulation Steps: 4 - Epoch [30/30] - Avg Train Loss: 0.0047, Val Loss: 0.0074, PSNR: 24.0042\n",
      "Hyperparameters (LR: 0.0005, Batch Size: 4, Accumulation Steps: 4) - Average PSNR over 30 epochs: 22.8701\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 1 - Epoch [1/30] - Avg Train Loss: 0.0165, Val Loss: 0.0178, PSNR: 19.3811\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 1 - Epoch [2/30] - Avg Train Loss: 0.0098, Val Loss: 0.0109, PSNR: 20.7812\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 1 - Epoch [3/30] - Avg Train Loss: 0.0091, Val Loss: 0.0109, PSNR: 20.8665\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 1 - Epoch [4/30] - Avg Train Loss: 0.0084, Val Loss: 0.0101, PSNR: 21.5101\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 1 - Epoch [5/30] - Avg Train Loss: 0.0079, Val Loss: 0.0103, PSNR: 21.8698\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 1 - Epoch [6/30] - Avg Train Loss: 0.0074, Val Loss: 0.0098, PSNR: 21.9314\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 1 - Epoch [7/30] - Avg Train Loss: 0.0073, Val Loss: 0.0094, PSNR: 22.1781\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 1 - Epoch [8/30] - Avg Train Loss: 0.0068, Val Loss: 0.0090, PSNR: 22.8638\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 1 - Epoch [9/30] - Avg Train Loss: 0.0069, Val Loss: 0.0080, PSNR: 22.4845\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 1 - Epoch [10/30] - Avg Train Loss: 0.0064, Val Loss: 0.0089, PSNR: 22.3335\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 1 - Epoch [11/30] - Avg Train Loss: 0.0064, Val Loss: 0.0091, PSNR: 22.7848\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 1 - Epoch [12/30] - Avg Train Loss: 0.0060, Val Loss: 0.0095, PSNR: 22.5127\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 1 - Epoch [13/30] - Avg Train Loss: 0.0060, Val Loss: 0.0090, PSNR: 22.4723\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 1 - Epoch [14/30] - Avg Train Loss: 0.0054, Val Loss: 0.0087, PSNR: 22.2701\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 1 - Epoch [15/30] - Avg Train Loss: 0.0052, Val Loss: 0.0087, PSNR: 22.5421\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 1 - Epoch [16/30] - Avg Train Loss: 0.0053, Val Loss: 0.0097, PSNR: 22.8432\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 1 - Epoch [17/30] - Avg Train Loss: 0.0049, Val Loss: 0.0082, PSNR: 23.2464\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 1 - Epoch [18/30] - Avg Train Loss: 0.0048, Val Loss: 0.0087, PSNR: 22.2409\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 1 - Epoch [19/30] - Avg Train Loss: 0.0049, Val Loss: 0.0080, PSNR: 22.8854\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 1 - Epoch [20/30] - Avg Train Loss: 0.0048, Val Loss: 0.0085, PSNR: 23.6209\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 1 - Epoch [21/30] - Avg Train Loss: 0.0046, Val Loss: 0.0087, PSNR: 21.9037\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 1 - Epoch [22/30] - Avg Train Loss: 0.0043, Val Loss: 0.0090, PSNR: 22.6031\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 1 - Epoch [23/30] - Avg Train Loss: 0.0042, Val Loss: 0.0083, PSNR: 23.3757\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 1 - Epoch [24/30] - Avg Train Loss: 0.0041, Val Loss: 0.0083, PSNR: 23.3104\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 1 - Epoch [25/30] - Avg Train Loss: 0.0041, Val Loss: 0.0081, PSNR: 23.1013\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 1 - Epoch [26/30] - Avg Train Loss: 0.0041, Val Loss: 0.0082, PSNR: 23.7504\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 1 - Epoch [27/30] - Avg Train Loss: 0.0041, Val Loss: 0.0086, PSNR: 23.0577\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 1 - Epoch [28/30] - Avg Train Loss: 0.0040, Val Loss: 0.0085, PSNR: 23.1608\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 1 - Epoch [29/30] - Avg Train Loss: 0.0039, Val Loss: 0.0086, PSNR: 23.1857\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 1 - Epoch [30/30] - Avg Train Loss: 0.0038, Val Loss: 0.0083, PSNR: 23.5769\n",
      "Hyperparameters (LR: 0.0005, Batch Size: 8, Accumulation Steps: 1) - Average PSNR over 30 epochs: 22.4882\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 4 - Epoch [1/30] - Avg Train Loss: 0.0271, Val Loss: 0.0162, PSNR: 18.8959\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 4 - Epoch [2/30] - Avg Train Loss: 0.0113, Val Loss: 0.0120, PSNR: 20.1491\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 4 - Epoch [3/30] - Avg Train Loss: 0.0095, Val Loss: 0.0105, PSNR: 21.7185\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 4 - Epoch [4/30] - Avg Train Loss: 0.0085, Val Loss: 0.0110, PSNR: 21.8958\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 4 - Epoch [5/30] - Avg Train Loss: 0.0083, Val Loss: 0.0104, PSNR: 21.7058\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 4 - Epoch [6/30] - Avg Train Loss: 0.0075, Val Loss: 0.0112, PSNR: 21.7033\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 4 - Epoch [7/30] - Avg Train Loss: 0.0075, Val Loss: 0.0096, PSNR: 21.7107\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 4 - Epoch [8/30] - Avg Train Loss: 0.0069, Val Loss: 0.0092, PSNR: 22.2022\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 4 - Epoch [9/30] - Avg Train Loss: 0.0068, Val Loss: 0.0087, PSNR: 22.6916\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 4 - Epoch [10/30] - Avg Train Loss: 0.0064, Val Loss: 0.0087, PSNR: 22.7728\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 4 - Epoch [11/30] - Avg Train Loss: 0.0064, Val Loss: 0.0085, PSNR: 22.0795\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 4 - Epoch [12/30] - Avg Train Loss: 0.0063, Val Loss: 0.0079, PSNR: 22.0934\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 4 - Epoch [13/30] - Avg Train Loss: 0.0059, Val Loss: 0.0085, PSNR: 22.6026\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 4 - Epoch [14/30] - Avg Train Loss: 0.0057, Val Loss: 0.0080, PSNR: 22.4296\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 4 - Epoch [15/30] - Avg Train Loss: 0.0056, Val Loss: 0.0077, PSNR: 22.6952\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 4 - Epoch [16/30] - Avg Train Loss: 0.0055, Val Loss: 0.0078, PSNR: 23.4003\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 4 - Epoch [17/30] - Avg Train Loss: 0.0052, Val Loss: 0.0077, PSNR: 22.9828\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 4 - Epoch [18/30] - Avg Train Loss: 0.0051, Val Loss: 0.0076, PSNR: 22.3226\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 4 - Epoch [19/30] - Avg Train Loss: 0.0050, Val Loss: 0.0079, PSNR: 23.0872\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 4 - Epoch [20/30] - Avg Train Loss: 0.0048, Val Loss: 0.0076, PSNR: 22.6442\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 4 - Epoch [21/30] - Avg Train Loss: 0.0046, Val Loss: 0.0078, PSNR: 23.3683\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 4 - Epoch [22/30] - Avg Train Loss: 0.0047, Val Loss: 0.0084, PSNR: 22.5950\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 4 - Epoch [23/30] - Avg Train Loss: 0.0044, Val Loss: 0.0073, PSNR: 22.8949\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 4 - Epoch [24/30] - Avg Train Loss: 0.0045, Val Loss: 0.0076, PSNR: 22.9518\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 4 - Epoch [25/30] - Avg Train Loss: 0.0042, Val Loss: 0.0074, PSNR: 23.6000\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 4 - Epoch [26/30] - Avg Train Loss: 0.0042, Val Loss: 0.0075, PSNR: 23.9568\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 4 - Epoch [27/30] - Avg Train Loss: 0.0042, Val Loss: 0.0071, PSNR: 23.2025\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 4 - Epoch [28/30] - Avg Train Loss: 0.0041, Val Loss: 0.0075, PSNR: 23.8773\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 4 - Epoch [29/30] - Avg Train Loss: 0.0041, Val Loss: 0.0074, PSNR: 23.0249\n",
      "LR: 0.0005, Batch Size: 8, Accumulation Steps: 4 - Epoch [30/30] - Avg Train Loss: 0.0040, Val Loss: 0.0075, PSNR: 23.2032\n",
      "Hyperparameters (LR: 0.0005, Batch Size: 8, Accumulation Steps: 4) - Average PSNR over 30 epochs: 22.4819\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 1 - Epoch [1/30] - Avg Train Loss: 0.0148, Val Loss: 0.0113, PSNR: 21.1792\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 1 - Epoch [2/30] - Avg Train Loss: 0.0112, Val Loss: 0.0126, PSNR: 20.6489\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 1 - Epoch [3/30] - Avg Train Loss: 0.0101, Val Loss: 0.0122, PSNR: 20.6648\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 1 - Epoch [4/30] - Avg Train Loss: 0.0094, Val Loss: 0.0120, PSNR: 21.6594\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 1 - Epoch [5/30] - Avg Train Loss: 0.0086, Val Loss: 0.0095, PSNR: 22.1860\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 1 - Epoch [6/30] - Avg Train Loss: 0.0080, Val Loss: 0.0087, PSNR: 22.3137\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 1 - Epoch [7/30] - Avg Train Loss: 0.0077, Val Loss: 0.0096, PSNR: 22.0910\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 1 - Epoch [8/30] - Avg Train Loss: 0.0076, Val Loss: 0.0079, PSNR: 23.0470\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 1 - Epoch [9/30] - Avg Train Loss: 0.0076, Val Loss: 0.0094, PSNR: 21.8734\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 1 - Epoch [10/30] - Avg Train Loss: 0.0070, Val Loss: 0.0081, PSNR: 22.8134\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 1 - Epoch [11/30] - Avg Train Loss: 0.0068, Val Loss: 0.0088, PSNR: 21.9325\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 1 - Epoch [12/30] - Avg Train Loss: 0.0066, Val Loss: 0.0090, PSNR: 22.1412\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 1 - Epoch [13/30] - Avg Train Loss: 0.0067, Val Loss: 0.0079, PSNR: 22.7553\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 1 - Epoch [14/30] - Avg Train Loss: 0.0063, Val Loss: 0.0082, PSNR: 23.1977\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 1 - Epoch [15/30] - Avg Train Loss: 0.0061, Val Loss: 0.0087, PSNR: 23.0719\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 1 - Epoch [16/30] - Avg Train Loss: 0.0062, Val Loss: 0.0079, PSNR: 22.7417\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 1 - Epoch [17/30] - Avg Train Loss: 0.0059, Val Loss: 0.0080, PSNR: 23.0906\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 1 - Epoch [18/30] - Avg Train Loss: 0.0057, Val Loss: 0.0075, PSNR: 23.0141\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 1 - Epoch [19/30] - Avg Train Loss: 0.0057, Val Loss: 0.0070, PSNR: 23.2524\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 1 - Epoch [20/30] - Avg Train Loss: 0.0055, Val Loss: 0.0077, PSNR: 22.6185\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 1 - Epoch [21/30] - Avg Train Loss: 0.0055, Val Loss: 0.0068, PSNR: 23.7511\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 1 - Epoch [22/30] - Avg Train Loss: 0.0054, Val Loss: 0.0080, PSNR: 22.8723\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 1 - Epoch [23/30] - Avg Train Loss: 0.0053, Val Loss: 0.0081, PSNR: 22.9103\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 1 - Epoch [24/30] - Avg Train Loss: 0.0052, Val Loss: 0.0080, PSNR: 23.2168\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 1 - Epoch [25/30] - Avg Train Loss: 0.0050, Val Loss: 0.0076, PSNR: 23.0813\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 1 - Epoch [26/30] - Avg Train Loss: 0.0050, Val Loss: 0.0069, PSNR: 23.8679\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 1 - Epoch [27/30] - Avg Train Loss: 0.0048, Val Loss: 0.0070, PSNR: 23.7248\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 1 - Epoch [28/30] - Avg Train Loss: 0.0049, Val Loss: 0.0067, PSNR: 23.8268\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 1 - Epoch [29/30] - Avg Train Loss: 0.0050, Val Loss: 0.0066, PSNR: 23.9324\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 1 - Epoch [30/30] - Avg Train Loss: 0.0049, Val Loss: 0.0075, PSNR: 23.6675\n",
      "Hyperparameters (LR: 0.001, Batch Size: 4, Accumulation Steps: 1) - Average PSNR over 30 epochs: 22.7048\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 4 - Epoch [1/30] - Avg Train Loss: 0.0180, Val Loss: 0.0119, PSNR: 20.7356\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 4 - Epoch [2/30] - Avg Train Loss: 0.0103, Val Loss: 0.0093, PSNR: 22.0449\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 4 - Epoch [3/30] - Avg Train Loss: 0.0093, Val Loss: 0.0108, PSNR: 20.6343\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 4 - Epoch [4/30] - Avg Train Loss: 0.0091, Val Loss: 0.0107, PSNR: 21.6298\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 4 - Epoch [5/30] - Avg Train Loss: 0.0086, Val Loss: 0.0108, PSNR: 21.3714\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 4 - Epoch [6/30] - Avg Train Loss: 0.0082, Val Loss: 0.0095, PSNR: 21.7089\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 4 - Epoch [7/30] - Avg Train Loss: 0.0078, Val Loss: 0.0095, PSNR: 21.9097\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 4 - Epoch [8/30] - Avg Train Loss: 0.0074, Val Loss: 0.0085, PSNR: 22.4699\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 4 - Epoch [9/30] - Avg Train Loss: 0.0073, Val Loss: 0.0089, PSNR: 22.3415\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 4 - Epoch [10/30] - Avg Train Loss: 0.0071, Val Loss: 0.0089, PSNR: 22.6532\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 4 - Epoch [11/30] - Avg Train Loss: 0.0071, Val Loss: 0.0092, PSNR: 22.2076\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 4 - Epoch [12/30] - Avg Train Loss: 0.0069, Val Loss: 0.0091, PSNR: 22.1447\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 4 - Epoch [13/30] - Avg Train Loss: 0.0067, Val Loss: 0.0087, PSNR: 22.7379\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 4 - Epoch [14/30] - Avg Train Loss: 0.0064, Val Loss: 0.0082, PSNR: 22.5575\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 4 - Epoch [15/30] - Avg Train Loss: 0.0062, Val Loss: 0.0085, PSNR: 22.0914\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 4 - Epoch [16/30] - Avg Train Loss: 0.0062, Val Loss: 0.0088, PSNR: 22.5460\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 4 - Epoch [17/30] - Avg Train Loss: 0.0061, Val Loss: 0.0083, PSNR: 22.5348\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 4 - Epoch [18/30] - Avg Train Loss: 0.0059, Val Loss: 0.0082, PSNR: 22.4172\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 4 - Epoch [19/30] - Avg Train Loss: 0.0058, Val Loss: 0.0085, PSNR: 22.6871\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 4 - Epoch [20/30] - Avg Train Loss: 0.0058, Val Loss: 0.0076, PSNR: 22.9433\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 4 - Epoch [21/30] - Avg Train Loss: 0.0057, Val Loss: 0.0083, PSNR: 22.7749\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 4 - Epoch [22/30] - Avg Train Loss: 0.0055, Val Loss: 0.0076, PSNR: 23.1136\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 4 - Epoch [23/30] - Avg Train Loss: 0.0055, Val Loss: 0.0081, PSNR: 23.7491\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 4 - Epoch [24/30] - Avg Train Loss: 0.0054, Val Loss: 0.0077, PSNR: 22.8624\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 4 - Epoch [25/30] - Avg Train Loss: 0.0052, Val Loss: 0.0076, PSNR: 23.4709\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 4 - Epoch [26/30] - Avg Train Loss: 0.0051, Val Loss: 0.0072, PSNR: 23.2277\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 4 - Epoch [27/30] - Avg Train Loss: 0.0052, Val Loss: 0.0077, PSNR: 23.2104\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 4 - Epoch [28/30] - Avg Train Loss: 0.0050, Val Loss: 0.0075, PSNR: 23.1921\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 4 - Epoch [29/30] - Avg Train Loss: 0.0051, Val Loss: 0.0069, PSNR: 23.6495\n",
      "LR: 0.001, Batch Size: 4, Accumulation Steps: 4 - Epoch [30/30] - Avg Train Loss: 0.0051, Val Loss: 0.0080, PSNR: 22.9698\n",
      "Hyperparameters (LR: 0.001, Batch Size: 4, Accumulation Steps: 4) - Average PSNR over 30 epochs: 22.4862\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 1 - Epoch [1/30] - Avg Train Loss: 0.0154, Val Loss: 0.0178, PSNR: 19.4640\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 1 - Epoch [2/30] - Avg Train Loss: 0.0105, Val Loss: 0.0120, PSNR: 20.3077\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 1 - Epoch [3/30] - Avg Train Loss: 0.0094, Val Loss: 0.0116, PSNR: 21.6742\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 1 - Epoch [4/30] - Avg Train Loss: 0.0090, Val Loss: 0.0096, PSNR: 21.3412\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 1 - Epoch [5/30] - Avg Train Loss: 0.0084, Val Loss: 0.0097, PSNR: 21.5868\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 1 - Epoch [6/30] - Avg Train Loss: 0.0082, Val Loss: 0.0103, PSNR: 21.6000\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 1 - Epoch [7/30] - Avg Train Loss: 0.0075, Val Loss: 0.0100, PSNR: 22.0173\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 1 - Epoch [8/30] - Avg Train Loss: 0.0072, Val Loss: 0.0107, PSNR: 22.5833\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 1 - Epoch [9/30] - Avg Train Loss: 0.0065, Val Loss: 0.0083, PSNR: 22.3781\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 1 - Epoch [10/30] - Avg Train Loss: 0.0065, Val Loss: 0.0090, PSNR: 21.7395\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 1 - Epoch [11/30] - Avg Train Loss: 0.0062, Val Loss: 0.0095, PSNR: 22.5130\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 1 - Epoch [12/30] - Avg Train Loss: 0.0062, Val Loss: 0.0080, PSNR: 22.5630\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 1 - Epoch [13/30] - Avg Train Loss: 0.0060, Val Loss: 0.0081, PSNR: 23.0579\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 1 - Epoch [14/30] - Avg Train Loss: 0.0056, Val Loss: 0.0079, PSNR: 23.0477\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 1 - Epoch [15/30] - Avg Train Loss: 0.0058, Val Loss: 0.0092, PSNR: 22.8521\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 1 - Epoch [16/30] - Avg Train Loss: 0.0056, Val Loss: 0.0081, PSNR: 23.1860\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 1 - Epoch [17/30] - Avg Train Loss: 0.0053, Val Loss: 0.0088, PSNR: 21.8454\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 1 - Epoch [18/30] - Avg Train Loss: 0.0051, Val Loss: 0.0082, PSNR: 22.9472\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 1 - Epoch [19/30] - Avg Train Loss: 0.0047, Val Loss: 0.0083, PSNR: 22.7592\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 1 - Epoch [20/30] - Avg Train Loss: 0.0047, Val Loss: 0.0083, PSNR: 23.1500\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 1 - Epoch [21/30] - Avg Train Loss: 0.0045, Val Loss: 0.0075, PSNR: 23.4146\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 1 - Epoch [22/30] - Avg Train Loss: 0.0045, Val Loss: 0.0079, PSNR: 23.2319\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 1 - Epoch [23/30] - Avg Train Loss: 0.0043, Val Loss: 0.0077, PSNR: 23.2353\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 1 - Epoch [24/30] - Avg Train Loss: 0.0042, Val Loss: 0.0075, PSNR: 23.2775\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 1 - Epoch [25/30] - Avg Train Loss: 0.0041, Val Loss: 0.0078, PSNR: 22.7440\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 1 - Epoch [26/30] - Avg Train Loss: 0.0040, Val Loss: 0.0077, PSNR: 23.5923\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 1 - Epoch [27/30] - Avg Train Loss: 0.0041, Val Loss: 0.0077, PSNR: 23.2822\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 1 - Epoch [28/30] - Avg Train Loss: 0.0040, Val Loss: 0.0077, PSNR: 23.3180\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 1 - Epoch [29/30] - Avg Train Loss: 0.0040, Val Loss: 0.0074, PSNR: 23.3680\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 1 - Epoch [30/30] - Avg Train Loss: 0.0039, Val Loss: 0.0075, PSNR: 23.2786\n",
      "Hyperparameters (LR: 0.001, Batch Size: 8, Accumulation Steps: 1) - Average PSNR over 30 epochs: 22.5119\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 4 - Epoch [1/30] - Avg Train Loss: 0.0202, Val Loss: 0.0149, PSNR: 20.1663\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 4 - Epoch [2/30] - Avg Train Loss: 0.0103, Val Loss: 0.0109, PSNR: 20.5268\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 4 - Epoch [3/30] - Avg Train Loss: 0.0090, Val Loss: 0.0110, PSNR: 21.0460\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 4 - Epoch [4/30] - Avg Train Loss: 0.0082, Val Loss: 0.0101, PSNR: 21.7894\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 4 - Epoch [5/30] - Avg Train Loss: 0.0077, Val Loss: 0.0103, PSNR: 21.3196\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 4 - Epoch [6/30] - Avg Train Loss: 0.0075, Val Loss: 0.0100, PSNR: 21.3140\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 4 - Epoch [7/30] - Avg Train Loss: 0.0072, Val Loss: 0.0095, PSNR: 21.8861\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 4 - Epoch [8/30] - Avg Train Loss: 0.0069, Val Loss: 0.0099, PSNR: 21.9818\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 4 - Epoch [9/30] - Avg Train Loss: 0.0068, Val Loss: 0.0095, PSNR: 21.7719\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 4 - Epoch [10/30] - Avg Train Loss: 0.0065, Val Loss: 0.0090, PSNR: 21.7144\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 4 - Epoch [11/30] - Avg Train Loss: 0.0063, Val Loss: 0.0082, PSNR: 22.6984\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 4 - Epoch [12/30] - Avg Train Loss: 0.0060, Val Loss: 0.0098, PSNR: 22.4405\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 4 - Epoch [13/30] - Avg Train Loss: 0.0060, Val Loss: 0.0088, PSNR: 22.5404\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 4 - Epoch [14/30] - Avg Train Loss: 0.0057, Val Loss: 0.0088, PSNR: 22.2876\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 4 - Epoch [15/30] - Avg Train Loss: 0.0055, Val Loss: 0.0095, PSNR: 22.1146\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 4 - Epoch [16/30] - Avg Train Loss: 0.0054, Val Loss: 0.0084, PSNR: 22.6380\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 4 - Epoch [17/30] - Avg Train Loss: 0.0052, Val Loss: 0.0089, PSNR: 22.5333\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 4 - Epoch [18/30] - Avg Train Loss: 0.0050, Val Loss: 0.0084, PSNR: 22.7083\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 4 - Epoch [19/30] - Avg Train Loss: 0.0051, Val Loss: 0.0085, PSNR: 22.7914\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 4 - Epoch [20/30] - Avg Train Loss: 0.0050, Val Loss: 0.0089, PSNR: 21.7064\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 4 - Epoch [21/30] - Avg Train Loss: 0.0048, Val Loss: 0.0083, PSNR: 22.3250\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 4 - Epoch [22/30] - Avg Train Loss: 0.0048, Val Loss: 0.0087, PSNR: 22.3185\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 4 - Epoch [23/30] - Avg Train Loss: 0.0046, Val Loss: 0.0083, PSNR: 22.8043\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 4 - Epoch [24/30] - Avg Train Loss: 0.0046, Val Loss: 0.0087, PSNR: 22.6018\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 4 - Epoch [25/30] - Avg Train Loss: 0.0044, Val Loss: 0.0086, PSNR: 22.5892\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 4 - Epoch [26/30] - Avg Train Loss: 0.0043, Val Loss: 0.0088, PSNR: 22.4489\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 4 - Epoch [27/30] - Avg Train Loss: 0.0043, Val Loss: 0.0084, PSNR: 22.7653\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 4 - Epoch [28/30] - Avg Train Loss: 0.0042, Val Loss: 0.0086, PSNR: 23.2313\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 4 - Epoch [29/30] - Avg Train Loss: 0.0043, Val Loss: 0.0085, PSNR: 22.6625\n",
      "LR: 0.001, Batch Size: 8, Accumulation Steps: 4 - Epoch [30/30] - Avg Train Loss: 0.0042, Val Loss: 0.0086, PSNR: 22.8769\n",
      "Hyperparameters (LR: 0.001, Batch Size: 8, Accumulation Steps: 4) - Average PSNR over 30 epochs: 22.1533\n",
      "Best Hyperparameters: {'lr': 0.0005, 'batch_size': 4, 'accumulation_steps': 1}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from skimage.metrics import peak_signal_noise_ratio\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Function to apply gamma correction to a numpy array image\n",
    "def apply_gamma_correction(img, gamma):\n",
    "    inv_gamma = 1.0 / gamma\n",
    "    table = np.array([(i / 255.0) ** inv_gamma * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "    return cv2.LUT(img, table)\n",
    "\n",
    "# Custom Dataset Class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, low_image_paths, high_image_paths, transform=None, gamma=None):\n",
    "        self.low_image_paths = low_image_paths\n",
    "        self.high_image_paths = high_image_paths\n",
    "        self.transform = transform\n",
    "        self.gamma = gamma\n",
    "        assert len(self.low_image_paths) == len(self.high_image_paths), \"Low and High image directories must contain the same number of images.\"\n",
    "\n",
    "    def gamma_correction(self, image, gamma=1.0):\n",
    "        inv_gamma = 1.0 / gamma\n",
    "        table = np.array([(i / 255.0) ** inv_gamma * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        return cv2.LUT(image, table)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.low_image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        low_img_path = self.low_image_paths[idx]\n",
    "        high_img_path = self.high_image_paths[idx]\n",
    "        low_image = Image.open(low_img_path).convert('RGB')\n",
    "        high_image = Image.open(high_img_path).convert('RGB')\n",
    "\n",
    "        if self.gamma:\n",
    "            low_image_np = np.array(low_image)\n",
    "            high_image_np = np.array(high_image)\n",
    "            low_image_np = self.gamma_correction(low_image_np, self.gamma)\n",
    "            high_image_np = self.gamma_correction(high_image_np, self.gamma)\n",
    "            low_image = Image.fromarray(low_image_np)\n",
    "            high_image = Image.fromarray(high_image_np)\n",
    "\n",
    "        if self.transform:\n",
    "            low_image = self.transform(low_image)\n",
    "            high_image = self.transform(high_image)\n",
    "\n",
    "        return low_image, high_image\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.adjust_channels = nn.Conv2d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.adjust_channels:\n",
    "            residual = self.adjust_channels(residual)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Down, self).__init__()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.residual_block = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        x = self.residual_block(x)\n",
    "        return x\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Up, self).__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.residual_block = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.residual_block(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.inc = ResidualBlock(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 512)\n",
    "        self.down5 = Down(512, 512)\n",
    "        self.up1 = Up(1024, 512)\n",
    "        self.up2 = Up(1024, 256)\n",
    "        self.up3 = Up(512, 128)\n",
    "        self.up4 = Up(256, 64)\n",
    "        self.up5 = Up(128, 64)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x6 = self.down5(x5)\n",
    "        x = self.up1(x6, x5)\n",
    "        x = self.up2(x, x4)\n",
    "        x = self.up3(x, x3)\n",
    "        x = self.up4(x, x2)\n",
    "        x = self.up5(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return torch.sigmoid(logits)\n",
    "\n",
    "# Paths to your low and high light images\n",
    "train_low_image_directory = '/kaggle/input/loldataset-4/LOLdataset/our485/low'\n",
    "train_high_image_directory = '/kaggle/input/loldataset-4/LOLdataset/our485/high'\n",
    "eval_low_image_directory = '/kaggle/input/loldataset-4/LOLdataset/eval15/low'\n",
    "eval_high_image_directory = '/kaggle/input/loldataset-4/LOLdataset/eval15/high'\n",
    "\n",
    "# Collecting image paths\n",
    "train_low_image_paths = [os.path.join(train_low_image_directory, img) for img in os.listdir(train_low_image_directory) if img.endswith('.png')]\n",
    "train_high_image_paths = [os.path.join(train_high_image_directory, img) for img in os.listdir(train_high_image_directory) if img.endswith('.png')]\n",
    "\n",
    "eval_low_image_paths = [os.path.join(eval_low_image_directory, img) for img in os.listdir(eval_low_image_directory) if img.endswith('.png')]\n",
    "eval_high_image_paths = [os.path.join(eval_high_image_directory, img) for img in os.listdir(eval_high_image_directory) if img.endswith('.png')]\n",
    "\n",
    "# Gamma correction value\n",
    "gamma_value = 2.2\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Datasets and DataLoaders\n",
    "train_dataset = CustomDataset(train_low_image_paths, train_high_image_paths, transform=transform, gamma=gamma_value)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "eval_dataset = CustomDataset(eval_low_image_paths, eval_high_image_paths, transform=transform, gamma=gamma_value)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# Model, Loss, Optimizer, and Scheduler\n",
    "num_models = 5\n",
    "models = [UNet(n_channels=3, n_classes=3).cuda() for _ in range(num_models)]\n",
    "criterion = nn.MSELoss()\n",
    "optimizers = [optim.AdamW(model.parameters(), lr=0.0001, weight_decay=1e-4) for model in models]\n",
    "schedulers = [optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5) for optimizer in optimizers]\n",
    "\n",
    "# Training and Evaluation Functions\n",
    "def train(model, criterion, optimizer, loader, scaler, accumulation_steps=1, max_norm=1.0):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for i, (low, high) in enumerate(loader):\n",
    "        low = low.cuda()\n",
    "        high = high.cuda()\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(low)\n",
    "            loss = criterion(outputs, high) / accumulation_steps\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.item() * accumulation_steps\n",
    "\n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    return epoch_loss\n",
    "\n",
    "def evaluate(model, criterion, loader):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    psnr_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for low, high in loader:\n",
    "            low = low.cuda()\n",
    "            high = high.cuda()\n",
    "\n",
    "            outputs = model(low)\n",
    "            loss = criterion(outputs, high)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            outputs = outputs.permute(0, 2, 3, 1).cpu().numpy()\n",
    "            high = high.permute(0, 2, 3, 1).cpu().numpy()\n",
    "\n",
    "            for i in range(outputs.shape[0]):\n",
    "                psnr_list.append(peak_signal_noise_ratio(high[i], outputs[i]))\n",
    "\n",
    "    avg_psnr = np.mean(psnr_list) if psnr_list else 0.0\n",
    "\n",
    "    return running_loss / len(loader), avg_psnr\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "param_grid = {\n",
    "    'lr': [0.0001, 0.0005, 0.001],\n",
    "    'batch_size': [4, 8],\n",
    "    'accumulation_steps': [1, 4]\n",
    "}\n",
    "\n",
    "best_params = {}\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "num_epochs = 30\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "for lr in param_grid['lr']:\n",
    "    for batch_size in param_grid['batch_size']:\n",
    "        for accumulation_steps in param_grid['accumulation_steps']:\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
    "            models = [UNet(n_channels=3, n_classes=3).cuda() for _ in range(num_models)]\n",
    "            optimizers = [optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4) for model in models]\n",
    "            schedulers = [optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5) for optimizer in optimizers]\n",
    "\n",
    "            # Track PSNR values for each epoch\n",
    "            all_psnr_values = []  # List to store PSNR values for all epochs\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                train_losses = []\n",
    "                for model, optimizer in zip(models, optimizers):\n",
    "                    train_loss = train(model, criterion, optimizer, train_loader, scaler, accumulation_steps=accumulation_steps, max_norm=1.0)\n",
    "                    train_losses.append(train_loss)\n",
    "\n",
    "                avg_train_loss = np.mean(train_losses)\n",
    "\n",
    "                val_losses = []\n",
    "                psnr_values = []\n",
    "                for model in models:\n",
    "                    val_loss, avg_psnr = evaluate(model, criterion, eval_loader)\n",
    "                    val_losses.append(val_loss)\n",
    "                    psnr_values.append(avg_psnr)\n",
    "\n",
    "                # Append the average PSNR of the best model for the current epoch\n",
    "                all_psnr_values.append(psnr_values[np.argmin(val_losses)])  \n",
    "\n",
    "                best_model_idx = np.argmin(val_losses)\n",
    "                val_loss = val_losses[best_model_idx]\n",
    "\n",
    "                print(f\"LR: {lr}, Batch Size: {batch_size}, Accumulation Steps: {accumulation_steps} - Epoch [{epoch+1}/{num_epochs}] - Avg Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, PSNR: {psnr_values[best_model_idx]:.4f}\")\n",
    "\n",
    "                for scheduler, val_loss in zip(schedulers, val_losses):\n",
    "                    scheduler.step(val_loss)\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_params = {\n",
    "                        'lr': lr,\n",
    "                        'batch_size': batch_size,\n",
    "                        'accumulation_steps': accumulation_steps\n",
    "                    }\n",
    "\n",
    "            avg_epoch_psnr = np.mean(all_psnr_values) if all_psnr_values else 0.0\n",
    "            print(f\"Hyperparameters (LR: {lr}, Batch Size: {batch_size}, Accumulation Steps: {accumulation_steps}) - Average PSNR over {num_epochs} epochs: {avg_epoch_psnr:.4f}\")\n",
    "\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eea4333",
   "metadata": {
    "papermill": {
     "duration": 0.034151,
     "end_time": "2024-06-17T23:33:07.912613",
     "exception": false,
     "start_time": "2024-06-17T23:33:07.878462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5198110,
     "sourceId": 8672927,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5221632,
     "sourceId": 8705339,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 35427.220938,
   "end_time": "2024-06-17T23:33:09.650390",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-17T13:42:42.429452",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
